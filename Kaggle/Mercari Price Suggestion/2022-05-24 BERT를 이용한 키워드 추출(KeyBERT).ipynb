{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b178fe",
   "metadata": {},
   "source": [
    "# 4) BERT를 이용한 키워드 추출 : 키버트(KeyBERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e601d839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\home\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.62.3)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.11.0-cp39-cp39-win_amd64.whl (157.9 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\home\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.20.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\home\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\home\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.7.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\home\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.6.5)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\home\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\home\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.0)\n",
      "Requirement already satisfied: requests in c:\\users\\home\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\home\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.8.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\home\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\home\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\home\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\home\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\home\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\home\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (8.4.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120748 sha256=a682a306f390a85a0a58eb9febb77b2b86cb4eda9f743a452bf301c4ea68e0d9\n",
      "  Stored in directory: c:\\users\\home\\appdata\\local\\pip\\cache\\wheels\\2b\\11\\3b\\32a18fb9f2253b25d3d1a06f0a84e2d516e7efa19c8c71a283\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: torch, tokenizers, huggingface-hub, transformers, torchvision, sentencepiece, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.6.0 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 torch-1.11.0 torchvision-0.12.0 transformers-4.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09bca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99bc1073",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input to an output based \n",
    "         on example input-output pairs.[1] It infers a function \n",
    "         from labeled training data consisting of a set of \n",
    "         training examples.[2] In supervised learning, each \n",
    "         example is a pair consisting of an input object \n",
    "         (typically a vector) and a desired output value (also \n",
    "         called the supervisory signal). A supervised learning \n",
    "         algorithm analyzes the training data and produces an \n",
    "         inferred function, which can be used for mapping new \n",
    "         examples. An optimal scenario will allow for the algorithm \n",
    "         to correctly determine the class labels for unseen \n",
    "         instances. This requires the learning algorithm to  \n",
    "         generalize from the training data to unseen situations \n",
    "         in a 'reasonable' way (see inductive bias).\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb96773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 : 72\n",
      "trigram 다섯개만 출력 : ['algorithm analyzes training' 'algorithm correctly determine'\n",
      " 'algorithm generalize training' 'allow algorithm correctly'\n",
      " 'analyzes training data']\n"
     ]
    }
   ],
   "source": [
    "# 3개의 단어 묶음인 단어구 추출\n",
    "n_gram_range = (3, 3)\n",
    "stop_words = \"english\"\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :',len(candidates))\n",
    "print('trigram 다섯개만 출력 :',candidates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f1c763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d6fda82e8546008e394db4341894c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af82b73fdf66447d8d8752cfb74e46fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cb134beeeb44da8a6edcc4b52b7962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18a884614304db1bad991ca9c1859b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/550 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c8b2aa8fa94c5cb6d4616f52e2aa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ee3793d6fc4add8fb28275cd3122ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b7576e9c1542eb956f631631c9b552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa49d0ade484ca9be1e8bcdeeb97b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff98df61eca64993a5e4f5fa5e8b76cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edcf0942c57461ab6c3c0c357123d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182cf830246d4fc081df67cec352e87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c744cfa3794a40bc4f5c965b7fee06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc]) # 원문\n",
    "candidate_embeddings = model.encode(candidates) # 주제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ad0277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algorithm analyzes training', 'learning algorithm generalize', 'learning machine learning', 'learning algorithm analyzes', 'algorithm generalize training']\n"
     ]
    }
   ],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6d682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c7a7849",
   "metadata": {},
   "source": [
    "## Max Sum Similarity\n",
    "\n",
    "데이터 쌍 사이의 최대 합 거리는 데이터 쌍 간의 거리가 최대화되는 데이터 쌍으로 정의됩니다. 여기서의 의도는 후보 간의 유사성을 최소화하면서 문서와의 후보 유사성을 극대화하고자 하는 것입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54c517f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6,7,8,9]\n",
    "a[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c56bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # 문서와 각 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1beba14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['requires learning algorithm',\n",
       " 'signal supervised learning',\n",
       " 'learning function maps',\n",
       " 'algorithm analyzes training',\n",
       " 'learning machine learning']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565399e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['set training examples',\n",
       " 'generalize training data',\n",
       " 'requires learning algorithm',\n",
       " 'supervised learning algorithm',\n",
       " 'learning machine learning']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3261b4e6",
   "metadata": {},
   "source": [
    "## 3. Maximal Marginal Relevance\n",
    "\n",
    "결과를 다양화하는 마지막 방법은 MMR(Maximum Limit Relegance)입니다. MMR은 텍스트 요약 작업에서 중복을 최소화하고 결과의 다양성을 극대화하기 위해 노력합니다. 참고 할 수 있는 자료로 EmbedRank(https://arxiv.org/pdf/1801.04470.pdf) 라는 키워드 추출 알고리즘은 키워드/키프레이즈를 다양화하는 데 사용할 수 있는 MMR을 구현했습니다. 먼저 문서와 가장 유사한 키워드/키프레이즈를 선택합니다. 그런 다음 문서와 유사하고 이미 선택된 키워드/키프레이즈와 유사하지 않은 새로운 후보를 반복적으로 선택합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3dc9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78bf0fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm generalize training',\n",
       " 'supervised learning algorithm',\n",
       " 'learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'learning algorithm generalize']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a658602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm generalize training',\n",
       " 'labels unseen instances',\n",
       " 'new examples optimal',\n",
       " 'determine class labels',\n",
       " 'supervised learning algorithm']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c56ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1922e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f58963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c47c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b0fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
