{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp # 병렬처리\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline # 사이킷런의 FeatureUnion을 사용하면 여러개의 pipeline을 하나의 pipeline으로 합칠 수도 있습니다\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "# CountVectorizer는 모든 작업을 메모리 상에서 수행하므로 처리할 문서의 크기가 커지면 속도가 느려지거나 실행이 불가능해진다. \n",
    "# 이 때 HashingVectorizer를 사용하면 해시 함수를 사용하여 단어에 대한 인덱스 번호를 생성하기 때문에 메모리 및 실행 시간을 줄일 수 있다.\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# 나만의 변환기 만들기\n",
    "# BaseEstimator를 상속하면 하이퍼파라미터 튜닝에 필요한 두 메서드 get_params()와 set_params()를 얻게 됩니다\n",
    "# fit_transform()은 TransformerMixin으로 구현\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype, is_integer_dtype, is_float_dtype, is_bool_dtype\n",
    "from pandas.api.types import is_object_dtype, is_string_dtype, is_categorical_dtype, is_datetime64_dtype\n",
    "# Pandas(판다스, 팬더스)에서 데이터타입이 숫자 또는 문자인지를 확인하는 법에 대해 알아보겠습니다.\n",
    "\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "mp.set_start_method('forkserver', True)\n",
    "# https://www.daleseo.com/python-os-environ/\n",
    "# 스레드 수(따라서 CPU 코어 수)\n",
    "# 포크 또는 소프트웨어 개발 포크, 프로젝트 포크는 개발자들이 하나의 소프트웨어 소스 코드를 통째로 복사하여 독립적인 새로운 소프트웨어를 개발하는 것을 말한다. 위키백과\n",
    "\n",
    "INPUT_PATH = r'../input'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268031a",
   "metadata": {},
   "source": [
    "## Damerau-Levenshtein distance(Edit distance)\n",
    "\n",
    "정보 이론 및 컴퓨터 과학에서 Damerau-Levenshtein 거리는 두 시퀀스 간의 편집 거리를 측정하기위한 문자열 메트릭입니다. \n",
    "\n",
    "비공식적으로 두 단어 사이의 Damerau-Levenshtein 거리는 한 단어를 다른 단어로 변경하는 데 필요한 최소 연산 수입니다. \n",
    "\n",
    "오타 수정 -> 문자열 비교 알고리즘\n",
    "문자열 A에서 몇 자(character)를 수정(delete, insert, substitute, transpose)하여 B가 되는걸 숫자로 표현한 것을 A와B의 Edit distance라고 부른다.\n",
    "transpose는 두개의 인접한 문자를 서로 바꾸는 것이다.  ex) lettre --> letter\n",
    "\n",
    "- L-distance(Levenshtein distance) : insert, delete, substitute\n",
    "- DL-distance(Damerau-Levenshtein distance) : insert, delete, substitute, transpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    This method has not been modified from the original.\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "\n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
    "    for x in range(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x + 1])\n",
    "        for y in range(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                    and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymSpell:\n",
    "    def __init__(self, max_edit_distance=3, verbose=0):\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.verbose = verbose\n",
    "        # 0: top suggestion\n",
    "        # 1: all suggestions of smallest edit distance\n",
    "        # 2: all suggestions <= max_edit_distance (slower, no early termination)\n",
    "\n",
    "        self.dictionary = {}\n",
    "        self.longest_word_length = 0\n",
    "\n",
    "    def get_deletes_list(self, w):\n",
    "        \"\"\"given a word, derive strings with up to max_edit_distance characters\n",
    "           deleted\"\"\"\n",
    "\n",
    "        deletes = []\n",
    "        queue = [w]\n",
    "        for d in range(self.max_edit_distance):\n",
    "            temp_queue = []\n",
    "            for word in queue:\n",
    "                if len(word) > 1:\n",
    "                    for c in range(len(word)):  # character index\n",
    "                        word_minus_c = word[:c] + word[c + 1:]\n",
    "                        if word_minus_c not in deletes:\n",
    "                            deletes.append(word_minus_c)\n",
    "                        if word_minus_c not in temp_queue:\n",
    "                            temp_queue.append(word_minus_c)\n",
    "            queue = temp_queue\n",
    "\n",
    "        return deletes\n",
    "\n",
    "    def create_dictionary_entry(self, w):\n",
    "        '''add word and its derived deletions to dictionary'''\n",
    "        # check if word is already in dictionary\n",
    "        # dictionary entries are in the form: (list of suggested corrections,\n",
    "        # frequency of word in corpus)\n",
    "        new_real_word_added = False\n",
    "        if w in self.dictionary:\n",
    "            # increment count of word in corpus\n",
    "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
    "        else:\n",
    "            self.dictionary[w] = ([], 1)\n",
    "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
    "\n",
    "        if self.dictionary[w][1] == 1:\n",
    "            # first appearance of word in corpus\n",
    "            # n.b. word may already be in dictionary as a derived word\n",
    "            # (deleting character from a real word)\n",
    "            # but counter of frequency of word in corpus is not incremented\n",
    "            # in those cases)\n",
    "            new_real_word_added = True\n",
    "            deletes = self.get_deletes_list(w)\n",
    "            for item in deletes:\n",
    "                if item in self.dictionary:\n",
    "                    # add (correct) word to delete's suggested correction list\n",
    "                    self.dictionary[item][0].append(w)\n",
    "                else:\n",
    "                    # note frequency of word in corpus is not incremented\n",
    "                    self.dictionary[item] = ([w], 0)\n",
    "\n",
    "        return new_real_word_added\n",
    "\n",
    "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        for line in arr:\n",
    "            # separate by words by non-alphabetical characters\n",
    "            words = re.findall(token_pattern, line.lower())\n",
    "            for word in words:\n",
    "                total_word_count += 1\n",
    "                if self.create_dictionary_entry(word):\n",
    "                    unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def create_dictionary(self, fname):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        with open(fname) as file:\n",
    "            for line in file:\n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', line.lower())\n",
    "                for word in words:\n",
    "                    total_word_count += 1\n",
    "                    if self.create_dictionary_entry(word):\n",
    "                        unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def get_suggestions(self, string, silent=False):\n",
    "        \"\"\"return list of suggested corrections for potentially incorrectly\n",
    "           spelled word\"\"\"\n",
    "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
    "            if not silent:\n",
    "                print(\"no items in dictionary within maximum edit distance\")\n",
    "            return []\n",
    "\n",
    "        suggest_dict = {}\n",
    "        min_suggest_len = float('inf')\n",
    "\n",
    "        queue = [string]\n",
    "        q_dictionary = {}  # items other than string that we've checked\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            q_item = queue[0]  # pop\n",
    "            queue = queue[1:]\n",
    "\n",
    "            # early exit\n",
    "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
    "                    ((len(string) - len(q_item)) > min_suggest_len)):\n",
    "                break\n",
    "\n",
    "            # process queue item\n",
    "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
    "                if self.dictionary[q_item][1] > 0:\n",
    "                    # word is in dictionary, and is a word from the corpus, and\n",
    "                    # not already in suggestion list so add to suggestion\n",
    "                    # dictionary, indexed by the word with value (frequency in\n",
    "                    # corpus, edit distance)\n",
    "                    # note q_items that are not the input string are shorter\n",
    "                    # than input string since only deletes are added (unless\n",
    "                    # manual dictionary corrections are added)\n",
    "                    assert len(string) >= len(q_item)\n",
    "                    suggest_dict[q_item] = (self.dictionary[q_item][1],\n",
    "                                            len(string) - len(q_item))\n",
    "                    # early exit\n",
    "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
    "                        break\n",
    "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
    "                        min_suggest_len = len(string) - len(q_item)\n",
    "\n",
    "                # the suggested corrections for q_item as stored in\n",
    "                # dictionary (whether or not q_item itself is a valid word\n",
    "                # or merely a delete) can be valid corrections\n",
    "                for sc_item in self.dictionary[q_item][0]:\n",
    "                    if sc_item not in suggest_dict:\n",
    "\n",
    "                        # compute edit distance\n",
    "                        # suggested items should always be longer\n",
    "                        # (unless manual corrections are added)\n",
    "                        assert len(sc_item) > len(q_item)\n",
    "\n",
    "                        # q_items that are not input should be shorter\n",
    "                        # than original string\n",
    "                        # (unless manual corrections added)\n",
    "                        assert len(q_item) <= len(string)\n",
    "\n",
    "                        if len(q_item) == len(string):\n",
    "                            assert q_item == string\n",
    "                            item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                        # item in suggestions list should not be the same as\n",
    "                        # the string itself\n",
    "                        assert sc_item != string\n",
    "\n",
    "                        # calculate edit distance using, for example,\n",
    "                        # Damerau-Levenshtein distance\n",
    "                        item_dist = dameraulevenshtein(sc_item, string)\n",
    "\n",
    "                        # do not add words with greater edit distance if\n",
    "                        # verbose setting not on\n",
    "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
    "                            pass\n",
    "                        elif item_dist <= self.max_edit_distance:\n",
    "                            assert sc_item in self.dictionary  # should already be in dictionary if in suggestion list\n",
    "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
    "                            if item_dist < min_suggest_len:\n",
    "                                min_suggest_len = item_dist\n",
    "\n",
    "                        # depending on order words are processed, some words\n",
    "                        # with different edit distances may be entered into\n",
    "                        # suggestions; trim suggestion dictionary if verbose\n",
    "                        # setting not on\n",
    "                        if self.verbose < 2:\n",
    "                            suggest_dict = {k: v for k, v in suggest_dict.items() if v[1] <= min_suggest_len}\n",
    "\n",
    "            # now generate deletes (e.g. a substring of string or of a delete)\n",
    "            # from the queue item\n",
    "            # as additional items to check -- add to end of queue\n",
    "            assert len(string) >= len(q_item)\n",
    "\n",
    "            # do not add words with greater edit distance if verbose setting\n",
    "            # is not on\n",
    "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
    "                pass\n",
    "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
    "                for c in range(len(q_item)):  # character index\n",
    "                    word_minus_c = q_item[:c] + q_item[c + 1:]\n",
    "                    if word_minus_c not in q_dictionary:\n",
    "                        queue.append(word_minus_c)\n",
    "                        q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "\n",
    "        # queue is now empty: convert suggestions in dictionary to\n",
    "        # list for output\n",
    "        if not silent and self.verbose != 0:\n",
    "            print(\"number of possible corrections: %i\" % len(suggest_dict))\n",
    "            print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "\n",
    "        # output option 1\n",
    "        # sort results by ascending order of edit distance and descending\n",
    "        # order of frequency\n",
    "        #     and return list of suggested word corrections only:\n",
    "        # return sorted(suggest_dict, key = lambda x:\n",
    "        #               (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "        # output option 2\n",
    "        # return list of suggestions with (correction,\n",
    "        #                                  (frequency in corpus, edit distance)):\n",
    "        as_list = suggest_dict.items()\n",
    "        # outlist = sorted(as_list, key=lambda (term, (freq, dist)): (dist, -freq))\n",
    "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
    "\n",
    "        if self.verbose == 0:\n",
    "            return outlist[0]\n",
    "        else:\n",
    "            return outlist\n",
    "\n",
    "        '''\n",
    "        Option 1:\n",
    "        ['file', 'five', 'fire', 'fine', ...]\n",
    "\n",
    "        Option 2:\n",
    "        [('file', (5, 0)),\n",
    "         ('five', (67, 1)),\n",
    "         ('fire', (54, 1)),\n",
    "         ('fine', (17, 1))...]  \n",
    "        '''\n",
    "\n",
    "    def best_word(self, s, silent=False):\n",
    "        try:\n",
    "            return self.get_suggestions(s, silent)[0]\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field, start_time=time()):\n",
    "        self.field = field\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        print(f'[{time()-self.start_time}] select {self.field}')\n",
    "        dt = dataframe[self.field].dtype\n",
    "        if is_categorical_dtype(dt):\n",
    "            return dataframe[self.field].cat.codes[:, None]\n",
    "        elif is_numeric_dtype(dt):\n",
    "            return dataframe[self.field][:, None]\n",
    "        else:\n",
    "            return dataframe[self.field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumnsByDf(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_df=1, max_df=1.0):\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        m = X.tocsc()\n",
    "        self.nnz_cols = ((m != 0).sum(axis=0) >= self.min_df).A1\n",
    "        if self.max_df < 1.0:\n",
    "            max_df = m.shape[0] * self.max_df\n",
    "            self.nnz_cols = self.nnz_cols & ((m != 0).sum(axis=0) <= max_df).A1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        m = X.tocsc()\n",
    "        return m[:, self.nnz_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f670df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cat(text):\n",
    "    try:\n",
    "        cats = text.split(\"/\")\n",
    "        return cats[0], cats[1], cats[2], cats[0] + '/' + cats[1]\n",
    "    except:\n",
    "        print(\"no category\")\n",
    "        return 'other', 'other', 'other', 'other/other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51125560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brands_filling(dataset):\n",
    "    vc = dataset['brand_name'].value_counts()\n",
    "    brands = vc[vc > 0].index\n",
    "    brand_word = r\"[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\"\n",
    "\n",
    "    many_w_brands = brands[brands.str.contains(' ')]\n",
    "    one_w_brands = brands[~brands.str.contains(' ')]\n",
    "\n",
    "    ss2 = SymSpell(max_edit_distance=0)\n",
    "    ss2.create_dictionary_from_arr(many_w_brands, token_pattern=r'.+')\n",
    "\n",
    "    ss1 = SymSpell(max_edit_distance=0)\n",
    "    ss1.create_dictionary_from_arr(one_w_brands, token_pattern=r'.+')\n",
    "\n",
    "    two_words_re = re.compile(r\"(?=(\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+))\")\n",
    "\n",
    "    def find_in_str_ss2(row):\n",
    "        for doc_word in two_words_re.finditer(row):\n",
    "            print(doc_word)\n",
    "            suggestion = ss2.best_word(doc_word.group(1), silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word.group(1)\n",
    "        return ''\n",
    "\n",
    "    def find_in_list_ss1(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss1.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "\n",
    "    def find_in_list_ss2(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss2.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "\n",
    "    print(f\"Before empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "\n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_name]\n",
    "\n",
    "    n_desc = dataset[dataset['brand_name'] == '']['item_description'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_desc]\n",
    "\n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in n_name]\n",
    "\n",
    "    desc_lower = dataset[dataset['brand_name'] == '']['item_description'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in desc_lower]\n",
    "\n",
    "    print(f\"After empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "\n",
    "    del ss1, ss2\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_regex(dataset, start_time=time()):\n",
    "    karats_regex = r'(\\d)([\\s-]?)(karat|karats|carat|carats|kt)([^\\w])'\n",
    "    karats_repl = r'\\1k\\4'\n",
    "\n",
    "    unit_regex = r'(\\d+)[\\s-]([a-z]{2})(\\s)'\n",
    "    unit_repl = r'\\1\\2\\3'\n",
    "\n",
    "    dataset['name'] = dataset['name'].str.replace(karats_regex, karats_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(karats_regex, karats_repl)\n",
    "    print(f'[{time() - start_time}] Karats normalized.')\n",
    "\n",
    "    dataset['name'] = dataset['name'].str.replace(unit_regex, unit_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(unit_regex, unit_repl)\n",
    "    print(f'[{time() - start_time}] Units glued.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pandas(train, test, start_time=time()):\n",
    "    train = train[train.price > 0.0].reset_index(drop=True)\n",
    "    print('Train shape without zero price: ', train.shape)\n",
    "\n",
    "    nrow_train = train.shape[0]\n",
    "    y_train = np.log1p(train[\"price\"])\n",
    "    merge: pd.DataFrame = pd.concat([train, test])\n",
    "\n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "\n",
    "    merge['has_category'] = (merge['category_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_category filled.')\n",
    "\n",
    "    merge['category_name'] = merge['category_name'] \\\n",
    "        .fillna('other/other/other') \\\n",
    "        .str.lower() \\\n",
    "        .astype(str)\n",
    "    merge['general_cat'], merge['subcat_1'], merge['subcat_2'], merge['gen_subcat1'] = \\\n",
    "        zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "    print(f'[{time() - start_time}] Split categories completed.')\n",
    "\n",
    "    merge['has_brand'] = (merge['brand_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_brand filled.')\n",
    "\n",
    "    merge['gencat_cond'] = merge['general_cat'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_1_cond'] = merge['subcat_1'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_2_cond'] = merge['subcat_2'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    print(f'[{time() - start_time}] Categories and item_condition_id concancenated.')\n",
    "\n",
    "    merge['name'] = merge['name'] \\\n",
    "        .fillna('') \\\n",
    "        .str.lower() \\\n",
    "        .astype(str)\n",
    "    merge['brand_name'] = merge['brand_name'] \\\n",
    "        .fillna('') \\\n",
    "        .str.lower() \\\n",
    "        .astype(str)\n",
    "    merge['item_description'] = merge['item_description'] \\\n",
    "        .fillna('') \\\n",
    "        .str.lower() \\\n",
    "        .replace(to_replace='No description yet', value='')\n",
    "    print(f'[{time() - start_time}] Missing filled.')\n",
    "\n",
    "    preprocess_regex(merge, start_time)\n",
    "\n",
    "    brands_filling(merge)\n",
    "    print(f'[{time() - start_time}] Brand name filled.')\n",
    "\n",
    "    merge['name'] = merge['name'] + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Name concancenated.')\n",
    "\n",
    "    merge['item_description'] = merge['item_description'] \\\n",
    "                                + ' ' + merge['name'] \\\n",
    "                                + ' ' + merge['subcat_1'] \\\n",
    "                                + ' ' + merge['subcat_2'] \\\n",
    "                                + ' ' + merge['general_cat'] \\\n",
    "                                + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Item description concatenated.')\n",
    "\n",
    "    merge.drop(['price', 'test_id', 'train_id'], axis=1, inplace=True)\n",
    "\n",
    "    return merge, y_train, nrow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_drop_columns(train: csr_matrix, valid: csr_matrix, min_df=0):\n",
    "    t = train.tocsc()\n",
    "    v = valid.tocsc()\n",
    "    nnz_train = ((t != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_valid = ((v != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_cols = nnz_train & nnz_valid\n",
    "    res = t[:, nnz_cols], v[:, nnz_cols]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658126ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('forkserver', True)\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    train = pd.read_table(os.path.join(INPUT_PATH, 'train.tsv'),\n",
    "                          engine='c',\n",
    "                          dtype={'item_condition_id': 'category',\n",
    "                                 'shipping': 'category'}\n",
    "                          )\n",
    "    test = pd.read_table(os.path.join(INPUT_PATH, 'test.tsv'),\n",
    "                         engine='c',\n",
    "                         dtype={'item_condition_id': 'category',\n",
    "                                'shipping': 'category'}\n",
    "                         )\n",
    "    print(f'[{time() - start_time}] Finished to load data')\n",
    "    print('Train shape: ', train.shape)\n",
    "    print('Test shape: ', test.shape)\n",
    "\n",
    "    submission: pd.DataFrame = test[['test_id']]\n",
    "\n",
    "    merge, y_train, nrow_train = preprocess_pandas(train, test, start_time)\n",
    "\n",
    "    meta_params = {'name_ngram': (1, 2),\n",
    "                   'name_max_f': 75000,\n",
    "                   'name_min_df': 10,\n",
    "\n",
    "                   'category_ngram': (2, 3),\n",
    "                   'category_token': '.+',\n",
    "                   'category_min_df': 10,\n",
    "\n",
    "                   'brand_min_df': 10,\n",
    "\n",
    "                   'desc_ngram': (1, 3),\n",
    "                   'desc_max_f': 150000,\n",
    "                   'desc_max_df': 0.5,\n",
    "                   'desc_min_df': 10}\n",
    "\n",
    "    stopwords = frozenset(['the', 'a', 'an', 'is', 'it', 'this', ])\n",
    "    # 'i', 'so', 'its', 'am', 'are'])\n",
    "\n",
    "    vectorizer = FeatureUnion([\n",
    "        ('name', Pipeline([\n",
    "            ('select', ItemSelector('name', start_time=start_time)),\n",
    "            ('transform', HashingVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                n_features=2 ** 27,\n",
    "                norm='l2',\n",
    "                lowercase=False,\n",
    "                stop_words=stopwords\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "        ])),\n",
    "        ('category_name', Pipeline([\n",
    "            ('select', ItemSelector('category_name', start_time=start_time)),\n",
    "            ('transform', HashingVectorizer(\n",
    "                ngram_range=(1, 1),\n",
    "                token_pattern='.+',\n",
    "                tokenizer=split_cat,\n",
    "                n_features=2 ** 27,\n",
    "                norm='l2',\n",
    "                lowercase=False\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "        ])),\n",
    "        ('brand_name', Pipeline([\n",
    "            ('select', ItemSelector('brand_name', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('gencat_cond', Pipeline([\n",
    "            ('select', ItemSelector('gencat_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_1_cond', Pipeline([\n",
    "            ('select', ItemSelector('subcat_1_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_2_cond', Pipeline([\n",
    "            ('select', ItemSelector('subcat_2_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('has_brand', Pipeline([\n",
    "            ('select', ItemSelector('has_brand', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('shipping', Pipeline([\n",
    "            ('select', ItemSelector('shipping', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_condition_id', Pipeline([\n",
    "            ('select', ItemSelector('item_condition_id', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_description', Pipeline([\n",
    "            ('select', ItemSelector('item_description', start_time=start_time)),\n",
    "            ('hash', HashingVectorizer(\n",
    "                ngram_range=(1, 3),\n",
    "                n_features=2 ** 27,\n",
    "                dtype=np.float32,\n",
    "                norm='l2',\n",
    "                lowercase=False,\n",
    "                stop_words=stopwords\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2)),\n",
    "        ]))\n",
    "    ], n_jobs=1)\n",
    "\n",
    "    sparse_merge = vectorizer.fit_transform(merge)\n",
    "    print(f'[{time() - start_time}] Merge vectorized')\n",
    "    print(sparse_merge.shape)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "    X = tfidf_transformer.fit_transform(sparse_merge)\n",
    "    print(f'[{time() - start_time}] TF/IDF completed')\n",
    "\n",
    "    X_train = X[:nrow_train]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    X_test = X[nrow_train:]\n",
    "    del merge\n",
    "    del sparse_merge\n",
    "    del vectorizer\n",
    "    del tfidf_transformer\n",
    "    gc.collect()\n",
    "\n",
    "    X_train, X_test = intersect_drop_columns(X_train, X_test, min_df=1)\n",
    "    print(f'[{time() - start_time}] Drop only in train or test cols: {X_train.shape[1]}')\n",
    "    gc.collect()\n",
    "\n",
    "    ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    print(f'[{time() - start_time}] Train Ridge completed. Iterations: {ridge.n_iter_}')\n",
    "\n",
    "    predsR = ridge.predict(X_test)\n",
    "    print(f'[{time() - start_time}] Predict Ridge completed.')\n",
    "\n",
    "    submission.loc[:, 'price'] = np.expm1(predsR)\n",
    "    submission.loc[submission['price'] < 0.0, 'price'] = 0.0\n",
    "    submission.to_csv(\"submission_ridge.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a550c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b1a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684ea3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db532a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b8da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
